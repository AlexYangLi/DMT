\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{DBLP:journals/corr/FengXGWZ15}
\citation{D17-1047}
\citation{NIPS2017_7181}
\citation{ACLP17-1152}
\newlabel{introduction}{{1}{1}{Introduction}{section.1}{}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{vocab differ}{{1}{1}{Different expressions with the same meaning used in Mainland China and Taiwan. \relax }{table.caption.1}{}}
\citation{W16-4801,W17-1201,W18-3901}
\citation{U13-1003}
\citation{Zampieri2012AutomaticIO}
\citation{Ciobanu2016ACP}
\citation{W17-1221}
\citation{W18-3922}
\citation{W18-3906}
\citation{Y08-1042}
\citation{IJNLC2016:5}
\newlabel{related work}{{2}{2}{Related work}{section.2}{}}
\newlabel{data and methodlogy}{{3}{2}{Data and Methodology}{section.3}{}}
\newlabel{data}{{3.1}{2}{Data}{subsection.3.1}{}}
\newlabel{ml models}{{3.2}{2}{Machine Learning Models with n-gram features}{subsection.3.2}{}}
\citation{DBLP:journals/corr/abs-1301-3781}
\citation{pennington2014glove}
\citation{bojanowski2017enriching}
\citation{D14-1181}
\citation{P14-1062}
\newlabel{data staticstics}{{2}{3}{Statistics of dataset for each variety. Sentence lengths are calculated based on word-level tokens from training and validation set.\relax }{table.caption.2}{}}
\newlabel{framework}{{1}{3}{The overall framework of deep models.\relax }{figure.caption.3}{}}
\newlabel{dl models}{{3.3}{3}{Deep Learning Models with word embeddings}{subsection.3.3}{}}
\citation{P17-1052}
\citation{DBLP:journals/corr/BahdanauCB14,DBLP:journals/corr/VaswaniSPUJGKP17}
\citation{DBLP:journals/corr/LinFSYXZB17}
\citation{DBLP:journals/corr/ZhouSLL15b}
\newlabel{ensemble models}{{3.4}{4}{Ensemble Models}{subsection.3.4}{}}
\citation{DBLP:journals/corr/KingmaB14}
\bibdata{dmt}
\bibcite{DBLP:journals/corr/BahdanauCB14}{{1}{2014}{{Bahdanau et~al.}}{{Bahdanau, Cho, and Bengio}}}
\newlabel{experiments}{{4}{5}{Experiments}{section.4}{}}
\newlabel{conclusion and feature work}{{5}{5}{Conclusion and feature work}{section.5}{}}
\newlabel{single_ngram_p}{{2}{6}{Macro-weighted f1 scores for LR (red lines), SVM (green lines), MNB (blue lines) using character (solid lines) or word level (dotted lines) n-gram of different sizes as input, both on dataset of simplified (left) and traditional (right) version.\relax }{figure.caption.4}{}}
\newlabel{combine_ngram_p}{{3}{6}{Macro-weighted f1 scores for LR, SVM, MNB using individual or combined features as input, both on dataset of simplified and traditional version.\relax }{table.caption.5}{}}
\newlabel{dl_model_p}{{4}{6}{Macro-weighted f1 scores for deep models using word embeddings as input, both on dataset of simplified and traditional version.\relax }{table.caption.6}{}}
\bibcite{bojanowski2017enriching}{{2}{2017}{{Bojanowski et~al.}}{{Bojanowski, Grave, Joulin, and Mikolov}}}
\bibcite{D17-1047}{{3}{2017{a}}{{Chen et~al.}}{{Chen, Sun, Bing, and Yang}}}
\bibcite{ACLP17-1152}{{4}{2017{b}}{{Chen et~al.}}{{Chen, Zhu, Ling, Wei, Jiang, and Inkpen}}}
\bibcite{Ciobanu2016ACP}{{5}{2016}{{Ciobanu and Dinu}}{{}}}
\bibcite{W17-1221}{{6}{2017}{{Clematide and Makarov}}{{}}}
\bibcite{W18-3906}{{7}{2018}{{{\c {C}}{\"o}ltekin et~al.}}{{{\c {C}}{\"o}ltekin, Rama, and Blaschke}}}
\bibcite{DBLP:journals/corr/FengXGWZ15}{{8}{2015}{{Feng et~al.}}{{Feng, Xiang, Glass, Wang, and Zhou}}}
\bibcite{Y08-1042}{{9}{2008}{{Huang and Lee}}{{}}}
\bibcite{P17-1052}{{10}{2017}{{Johnson and Zhang}}{{}}}
\bibcite{P14-1062}{{11}{2014}{{Kalchbrenner et~al.}}{{Kalchbrenner, Grefenstette, and Blunsom}}}
\bibcite{D14-1181}{{12}{2014}{{Kim}}{{}}}
\bibcite{DBLP:journals/corr/KingmaB14}{{13}{2014}{{Kingma and Ba}}{{}}}
\bibcite{W18-3922}{{14}{2018}{{Kreutz and Daelemans}}{{}}}
\bibcite{DBLP:journals/corr/LinFSYXZB17}{{15}{2017}{{Lin et~al.}}{{Lin, Feng, dos Santos, Yu, Xiang, Zhou, and Bengio}}}
\bibcite{U13-1003}{{16}{2013}{{Lui and Cook}}{{}}}
\newlabel{ensemble_model_p}{{5}{7}{Macro-weighted f1 scores for 4 ensemble strategies combining different base classifiers, both on dataset of simplified and traditional version. ``all ML'' and ``all DL'' refer to combine all machine learning models and deep learning models respectively. All machine learning models use character bigram-trigram combination as input.\relax }{table.caption.7}{}}
\bibcite{W16-4801}{{17}{2016}{{Malmasi et~al.}}{{Malmasi, Zampieri, Ljube{\v {s}}i{\'{c}}, Nakov, Ali, and Tiedemann}}}
\bibcite{DBLP:journals/corr/abs-1301-3781}{{18}{2013}{{Mikolov et~al.}}{{Mikolov, Chen, Corrado, and Dean}}}
\bibcite{pennington2014glove}{{19}{2014}{{Pennington et~al.}}{{Pennington, Socher, and Manning}}}
\bibcite{NIPS2017_7181}{{20}{2017{a}}{{Vaswani et~al.}}{{Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin}}}
\bibcite{DBLP:journals/corr/VaswaniSPUJGKP17}{{21}{2017{b}}{{Vaswani et~al.}}{{Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin}}}
\bibcite{IJNLC2016:5}{{22}{2016}{{Xu et~al.}}{{Xu, Wang, and Li}}}
\bibcite{Zampieri2012AutomaticIO}{{23}{2012}{{Zampieri and Gebre}}{{}}}
\bibcite{W17-1201}{{24}{2017}{{Zampieri et~al.}}{{Zampieri, Malmasi, Ljube{\v {s}}i{\'{c}}, Nakov, Ali, Tiedemann, Scherrer, and Aepli}}}
\bibcite{W18-3901}{{25}{2018}{{Zampieri et~al.}}{{Zampieri, Malmasi, Nakov, Ali, Shon, Glass, Scherrer, Samard{\v {z}}i{\'{c}}, Ljube{\v {s}}i{\'{c}}, Tiedemann, van~der Lee, Grondelaers, Oostdijk, Speelman, van~den Bosch, Kumar, Lahiri, and Jain}}}
\bibcite{DBLP:journals/corr/ZhouSLL15b}{{26}{2015}{{Zhou et~al.}}{{Zhou, Sun, Liu, and Lau}}}
\bibstyle{acl_natbib}
